# Today I learned...

This is a running list to add to daily, to track my progress.

## 2018

### September

| Date          | Today I learned...                             | Streak |
|:--------------|:-----------------------------------------------|--------|
| **W&nbsp;19** | Today was networking! I met up with S\_\_ of C\_\_ for an AMA one-on-one session, and then in the evening went to Shopify's Intern Open House. I read a bit more from the _Cookbook_ but no time for actual sitting down to code. S\_\_ suggested I check out **Open AI Gym**. Also, she gave me an idea for a project while we were talking, an unconcious bias detector to detect bias in the dataset that's going to go into an unconcious bias detector... ‚ôªÔ∏è | 3 |
| **T&nbsp;18** | Today I inched closer to making CUDA work with **my\_first\_neural\_network** project using the _Python Parallel Programming Cookbook_ by Giancarlo Zaccone. The last chapter is about PyCUDA, bit dated because old Python version, but it's not a problem for the moment. Also learned/realized through the **Fast.ai** course that there are activation functions other than the sigmoid function, so I listed that in my Master Plan as a feature to work on after CUDA. | 2 |
| **M&nbsp;17** | *Rebooting streak...* So with Nina being sick ü§Æ, my ear being infected *again* üçÑüëÇ (it's been over 3 months off and on like this), √âtienne travelling üõ©, and *Project X* blowing up üí£, it's been a bit tough... But today I **1) made a Master Plan** specific for my learning; **2) started on the rest of the Shopify challenge** (using `pandas`); and **3) found an ebook that may very well help me get unstuck** trying to train my neural network using PyCUDA! Some of these Kindle books are amazing, the amount of wisdom they contain for less than $10... | 1 |
|               | Sunday Nina was feeling better and the team for *Project X* was working hard from our kitchen table. We sat down to draw at *La Finca*. Made good progress on a dreamy cat with flowers on its head. üêàüå∫üåºüå∏ |  |
| **T&nbsp;13** | Applied for a part-time job at a ML startup at Notman House, they're in the Techstars program... It's a job as a content sourcer, but I got in touch with the CTO in the context of me interviewing interesting people in the field while I'm working towards getting ready for an internship somewhere. This counts as a TIL, right? | 3 |
| **W&nbsp;12** | Today **I finished the SQL challenges** and I can now say that *I know SQL-fu.* I wrote my solutions in a markdown file that I can easily export to PDF and attach to the application. | 2 |
| **T&nbsp;11** | So I dropped off the map for a bit, restarting my streak today! üí™üèª Number to beat: 15 | 1 |
|               | Today I learned `SELECT * FROM Customers;` in SQL. I never had to learn SQL but now I have to, so I can do the code challenge for an internship application. Actually, the SQL challenge is so f-ed up that even if you did know SQL you still had to poke around in the documentation to figure it out. So, no point in actually doing a tutorial step-by-step, I'm bouncing around in the docs instead... I think this is probably what they want to see in your answers. |  |
| **T&nbsp;6**  | Today I'm good and well *stuck*. Not moving forward or backward. Don't know what thread to pull to be able to untangle... But supposedly that moment when you're stuck is when you're on the cusp of a breakthrough in your learning. Right...? I think the important part is to resist the urge switch to something else that's going to make me feel like I'm making progress, because that's just a trick your brain plays. Learning is supposed to be hard. If you're always making progress without ever getting stuck, you're not actually learning *and nothing sticks*. ;) | 15 |
| **W&nbsp;5**  | Today I got started on an *end-to-end machine learning project with scikit-learn*, using ***Hands-On ML*** as a guide. Haven't forgotten that I still haven't finished my neural network with CUDA... Working towards getting some peace and quiet during the daytime so that I can dig my way through it. | 14 |
|               | Also started contacting a few people to set up meetings with, I'd like to get an internship lined up in a few months. |  |
|               | Which brings me to the next point: I didn't mention it yesterday, but I worked on a **Master Plan** for the coming months. I want to get to a point where every day I focus on 3 main areas: 1. ML, 2. drawing and 3. gym/run/yoga. Weather from tomorrow onwards should be better, still hot today... |  |
| **T&nbsp;4**  | Today I learned about **precision** vs **recall**. There is a *precision/recall-tradeoff*. If your model's predictions have a high precision, that means it doesn't have a lot of false positives in the bunch of positive predictions. However, your model may have a high number of false negatives (low recall). The more precision, the less recall. | 13 |
|               | For example, you will want to make sure that a content filter for kids has a low number of false positives (high precision) so nothing slips through the cracks, even though some of the safe ones don't get past the filter. | |
|               | On the other hand, if you're airport security and you want to identify potential terrorists, you don't care if you scoop up mostly false positives of grannies carrying handsoap and nail clippers... üôÑ at least your recall is high! |  |
| **M&nbsp;3**  | Fingers all better, and finished the drawing. Going to bed with the _Hands-on ML_ book. Not sure if that should count towards my streak... but tomorrow back to regular schedule. All that stippling took me a lot of time, just for a drawing the size of a credit card... üí≥ Tiny habits are the best though! | 12 |
|               | One thing I learned from the _Hands-on ML_ book is best practises like when preparing and cleaning up your dataset, to always write functions to do so (rather than doing it manually). That way you can reproduce exactly what you did when new data comes in, even automate the process if your ML pipeline goes live. |  |
| **S&nbsp;2**  | Went kayaking today on the Saint-Laurent by √éles-de-Boucherville, and got soaked by the rain that started half an hour before we got back. So had an awesome day but I'm tired, and my old RSI is flaring up while I'm typing this, so I'm going to stay away from the computer. But I did **clone the github repo of the *Hands-on* book** to my computer. üèÖ | 11 |
| **S&nbsp;1**  | Tired today, but [**I spent all afternoon drawing**](https://www.instagram.com/p/BnMrr1GhmzK/). After LAB12 I'll have control over my own schedule again, and **my 3 core activities** should be **1) coding/ML, 2) drawing,** and **3) working out** (like running, and lifting heavy weights at the gym). I plan to participate in **#inktober** and want to use the month of September to get into a drawing routine. | 10 |
|               | So, with all that, booting up my Paperspace machine this evening and starting a new and potentially complicated task&mdash;I'm not feeling it. But I will take the **_Hands-on_** book to bed, I think that book is going to be better for me than any of the online university-level courses I've seen so far. |  |
|               | Oh, I also sent a message to Ugo and the DS group this morning that project-based learning is working out better for me than classroom learning and that I'm out of the Python class... |  |

### August

| Date          | Today I learned...                             | Streak |
|:--------------|:-----------------------------------------------|--------|
| **F&nbsp;31** | No time for much CUDA today but I did get this book and started reading it: [_Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems_](https://www.amazon.com/dp/B06XNKV5TS/ref=cm_sw_r_cp_api_eJFIBb0AMSAB5) | 9 |
| **T&nbsp;30** | Back to CUDA/PyCUDA, to the tune of [*Barracuda*](https://www.youtube.com/watch?v=hpkitLUbeEg). I'm so happy because **I finally (FINALLY!) managed to get basic CUDA sample code to compile!** I finally managed to tell the GPU to take an array of 400 ones and double each value in it, in parallel! Can't wait to feed it a few matrices from my neural net to chew on... *:insert_waffle_emoji_here:* I think CUDA just really liked the song, and so she was playing nice this evening. | 8 |
| **W&nbsp;29** | Yesterday I had to leave PyCUDA with a **compile issue**, so I had a hunch it might be the same issue I was having to get CUDA code to compile the day before... Today I learned that my CUDA/PyCUDA compile issues are caused by an old version (7.5) lingering on the system on Paperspace, even though I installed 9.1, the latest version. I fixed that and now I'm getting pointer errors, hurray! üéâ Still haven't managed to get any CUDA/PyCUDA sample code to run tho... üôÑ I'm hopeful for tomorrow! üí™üèª | 7 |
| **T&nbsp;28** | Today I learned that a bunch of 32 threads is called a **warp**. Also, I started working with **PyCUDA**, a Python wrapper around C-based CUDA. You still need to know CUDA to understand PyCUDA but this way I can actually talk to the GPU in my Python code. (The whole point of the CUDA rabbit hole is so that I can make my neural network learn faster by passing the computationally demanding code to the GPU.) | 6 |
| **M&nbsp;27** | Yesterday I came back from vacation (no computer!), and today I had to review a lot of the **CUDA** stuff I'd started before going camping. I wrote an exercise to copy data from the CPU memory to the GPU memory, but got a compile error before I had to leave to pick up kiddo. Relearned a few VIM keys to navigate in the code because I was ssh'ed into my remote machine. Got a bit frustrated because I had no fancy IDE with helpful red squiggles to indicate where the compile error is... | 5 |
| | Fun fact, when writing a function (a *kernel*) to run on GPU, it seems like you don't return anything. You create a pointer with allocated space in memory and pass that as a parameter together with the data that you want to do the calculation on, and then you write the result directly to memory using that pointer. ü§™ | |
| **F&nbsp;17** | Today I learned during our **LAB12 field trip** that **Shopify is opening up internships** again soon üôÑ but I also felt I should move forward a bit on my learning CUDA before midnight so: I finished the theoretical part of the video, and set up the remote Paperspace machine with the exercise files so that I'm ready to start doing actual CUDA code next. Command line access only so I'll need to brush up on my `vim` skills, it's been a while. | 4 |
| | Going camping this weekend ‚õ∫Ô∏è, don't know how I'll keep my streak going but I won't have internet access. I'll still try to do commits, and push when I'm back online. üë©‚Äçüíª | |
| **T&nbsp;16** | Today I continued the [**intro to CUDA**](https://www.youtube.com/watch?v=_41LCMFpsFs). Not writing actual code yet, just the instructor explaining how things work by starting from very simple code examples. You need to put your data on the GPU's memory, or your GPU won't know about it... And it's very much like C where you declare a variable, put a * before it to indicate it's a pointer to a location in memory, and then you allocate the actual space you need for it. Writing C is like riding a bike, you never forget it. | 3 |
| **W&nbsp;15** | Today I learned that if you roll-your-own neural network and put it up on a server that has an **NVIDIA GPU for machine learning**, it's not magically going to run faster when you train it üôÑ duh. (So I did put it up on **Paperspace**) | 2 |
| | I found a YouTube video [**Learn CUDA in an afternoon**](https://www.youtube.com/watch?v=_41LCMFpsFs) so that sounds promising. With the GPU you can do parallel processing if you need to do the same operation on a whole slew of data. CUDA is built on top of C so I'm not sure how I'm going to make that work in my Python code... (edit: of course, with **PyCUDA**...) | |
| | I also got back into the **[Fast.AI course](course.fast.ai)** | |
| **T&nbsp;14** | Back to my sweet **neural net** ‚ù§Ô∏è Today I finished my chapter on basic **calculus**. It's still a bit fuzzy to me but it'll help me understand a **partial derivative** (= a **gradient**), how a change in a single weight between two nodes affects the total error of the network. Getting to the meat of gradient descent now. (And I'm allowed to play EXAPUNKS before bed üòù) | 1 |
| **M&nbsp;13** | **Damn you EXAPUNKS**, I lost my TIL streak because all I want is to play that game üò´ Ok that's it, ***no EXAPUNKS before TIL.*** I'm only allowed to play for as long as I've coded or studied. | 0 |
| **F&nbsp;10** | Learned more about **assembly language** during our LAB12 session, and when I got home I installed **EXAPUNKS** to do some pretend-assembly in a pretend-1997 dystopia (90s-era phreaking zine included, reminiscent of _2600_) | 12 |
| **T&nbsp;09** | Got a tiny bit further with the **calculus for n00bs** chapter, I'll bring it to bed so that it will count fo realz. And I'll do a commit first. Oh! I learned how to fix the toilet seat back to the toilet after the new cleaner did such a good job that it came off üöΩ | 11 |
| **W&nbsp;08** | Today I started learning some **calculus** that I need to know to understand the math behind training a neural network. No commit tonight, I fell asleep instead.. | 10 |
| **T&nbsp;07** | I retrained my model using the **full training set**, and accuracy jumped from 60% to close to 95% accuracy! After playing with the **number of nodes** and the **learning rate**, as well as adding more **epochs**, I managed an accuracy of 97.35%! That's a 2.65% error rate -- compared with the historical benchmarks here it's very very decent! http://yann.lecun.com/exdb/mnist/ | 9 |
| **M&nbsp;06** | Today I **trained my model!!** With only 100 datapoints to train it and a hidden layer of only 100 nodes, it guesses right 6 out of 10 times using the test data. Let's see tomorrow if we can improve on that :) `W00t!` | 8 |
| **S&nbsp;05** | Today I wrote the `train()` method of my `neuralNetwork` class. The last bit of math, where the network learns from errors by updating the weights between the layers, is still fuzzy to me. I've left it like that for now, I need to learn some calculus basics to understand it but wanted to get my hands in the code too badly... Next up: feeding it data to actually train my model! | 7 |
| **S&nbsp;04** | Today I worked more on the [**neural network**](https://github.com/rvoulon/my-first-neural-network) I started coding last night before bed. | 6 |
|            | Also bought a keyboard and mouse and set up a new microSD with Nina for her Piper computer. We set up a new Raspbian install and next we want to do actual Python together. |   |
| **F&nbsp;03** | Today I actually understood something important about **gradient descent**. I'd already seen Andrew Ng's explanation about gradient descent but didn't get the point of it, and didn't understand why you needed it.. Usually gradient descent is explained with a visualization of some equation (with 2 or 3 dimensions), but I never understood why you needed something complicated like gradient descent to get the lowest value of *x* when you can just _point at it in the graph._ Why can't you just simply _solve the equation?_ | 5 |
|            | I finally understand why, it's because what was never mentioned is that those graphs with 2 or 3 dimensions are gross oversimplifications, because the actual equation is _n_-dimensional, it has as many dimensions as there are weights coming into a layer. So it's just utterly impossible to visualize, and mindblowingly difficult to solve. Therefore you need to use gradient descent to "gradually descend" down the slope of the graph until you've got a pretty close guess at the lowest value of x.  | ü§Ø |
| **T&nbsp;02** | Today I learned about **X** = **I** * **W**, where **X** is the matrix of all the moderated signals in a single layer of a neural network. **I** is the matrix of all the input signals coming from the previous layer (or coming into the input layer). **I** is moderated by **W**, the matrix of all the weights between this and the previous layer. After you've calculated **X**, you apply the sigmoid activation function to decide whether the signal is boosted enough to be sent to the next layer: output matrix **O** = sigmoid(**X**) | 4 |
|           | Today I learned and also about **backpropagation**, where an error in the output (as compared to the training data) is used to adjust the weights proportionally, going back layer by layer until all weights have been adjusted. This is how the model is trained on the training data. |  |
| **W&nbsp;01** | Today I learned about **sigmoid functions (logistic function)**, and multiplying **matrices**, and why it makes so much sense to express inputs and weights as matrices! (_Make Your Own Neural Network_)         | 3 |

### July

| Date          | Today I learned...                             | Streak |
|:--------------|:-----------------------------------------------|--------|
| **T&nbsp;31** | Today I learned that a **loss** or **cost function** is really a function to describe the difference (error) between a predicted value and a real world value... | 2 |
|               | Today I learned and that the **learning rate** is actually a fraction that moderates the change in parameters from one training example to the next, so that we don't lose the results from  all the previous training iterations.  (_Make Your Own Neural Network_) |   |
| **M&nbsp;30** | Today I learned how to set up **Github Pages**          | 1 |
