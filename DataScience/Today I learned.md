# Today I learned...

This is a running list to add to daily to track my progress.

## 2018

### August

| Date       | Today I learned...                             | Streak |
|:-----------|:-----------------------------------------------|   |
| **T&nbsp;28** | Today I learned that a bunch of 32 threads is called a **warp**. Also, I started working with **PyCUDA**, a Python wrapper around C-based CUDA. You still need to know CUDA to understand PyCUDA but this way I can actually talk to the GPU in my Python code. (The whole point of the CUDA rabbit hole is so that I can make my neural network learn faster by passing the computationally demanding code to the GPU.) | 6 |
| **M&nbsp;27** | Yesterday I came back from vacation (no computer!), and today I had to review a lot of the **CUDA** stuff I'd started before going camping. I wrote an exercise to copy data from the CPU memory to the GPU memory, but got a compile error before I had to leave to pick up kiddo. Relearned a few VIM keys to navigate in the code because I was ssh'ed into my remote machine. Got a bit frustrated because I had no fancy IDE with helpful red squiggles to indicate where the compile error is... | 5 |
| | Fun fact, when writing a function (a *kernel*) to run on GPU, it seems like you don't return anything. You create a pointer with allocated space in memory and pass that as a parameter together with the data that you want to do the calculation on, and then you write the result directly to memory using that pointer. ü§™ | |
| **F&nbsp;17** | Today I learned during our **LAB12 field trip** that **Shopify is opening up internships** again soon üôÑ but I also felt I should move forward a bit on my learning CUDA before midnight so: I finished the theoretical part of the video, and set up the remote Paperspace machine with the exercise files so that I'm ready to start doing actual CUDA code next. Command line access only so I'll need to brush up on my `vim` skills, it's been a while. | 4 |
| | Going camping this weekend ‚õ∫Ô∏è, don't know how I'll keep my streak going but I won't have internet access. I'll still try to do commits, and push when I'm back online. üë©‚Äçüíª | |
| **T&nbsp;16** | Today I continued the [**intro to CUDA**](https://www.youtube.com/watch?v=_41LCMFpsFs). Not writing actual code yet, just the instructor explaining how things work by starting from very simple code examples. You need to put your data on the GPU's memory, or your GPU won't know about it... And it's very much like C where you declare a variable, put a * before it to indicate it's a pointer to a location in memory, and then you allocate the actual space you need for it. Writing C is like riding a bike, you never forget it. | 3 |
| **W&nbsp;15** | Today I learned that if you roll-your-own neural network and put it up on a server that has an **NVIDIA GPU for machine learning**, it's not magically going to run faster when you train it üôÑ duh. (So I did put it up on **Paperspace**) | 2 |
| | I found a YouTube video [**Learn CUDA in an afternoon**](https://www.youtube.com/watch?v=_41LCMFpsFs) so that sounds promising. With the GPU you can do parallel processing if you need to do the same operation on a whole slew of data. CUDA is built on top of C so I'm not sure how I'm going to make that work in my Python code... (edit: of course, with **PyCUDA**...) | |
| | I also got back into the **[Fast.AI course](course.fast.ai)** | |
| **T&nbsp;14** | Back to my sweet **neural net** ‚ù§Ô∏è Today I finished my chapter on basic **calculus**. It's still a bit fuzzy to me but it'll help me understand a **partial derivative** (= a **gradient**), how a change in a single weight between two nodes affects the total error of the network. Getting to the meat of gradient descent now. (And I'm allowed to play EXAPUNKS before bed üòù) | 1 |
| **M&nbsp;13** | **Damn you EXAPUNKS**, I lost my TIL streak because all I want is to play that game üò´ Ok that's it, ***no EXAPUNKS before TIL.*** I'm only allowed to play for as long as I've coded or studied. | 0 |
| **F&nbsp;10** | Learned more about **assembly language** during our LAB12 session, and when I got home I installed **EXAPUNKS** to do some pretend-assembly in a pretend-1997 distopia (90s-era phreaking zine included, reminiscent of _2600_) | 12 |
| **T&nbsp;09** | Got a tiny bit further with the **calculus for n00bs** chapter, I'll bring it to bed so that it will count fo realz. And I'll do a commit first. Oh! I learned how to fix the toilet seat back to the toilet after the new cleaner did such a good job that it came off üöΩ | 11 |
| **W&nbsp;08** | Today I started learning some **calculus** that I need to know to understand the math behind training a neural network. No commit tonight, I fell asleep instead.. | 10 |
| **T&nbsp;07** | I retrained my model using the **full dataset**, and accuracy jumped from 60% to close to 95% accuracy! After playing with the **number of nodes** and the **learning rate**, as well as adding more **epochs**, I managed an accuracy of 97.35%! That's a 2.65% error rate -- compared with the historical benchmarks here it's very very decent! http://yann.lecun.com/exdb/mnist/ | 9 |
| **M&nbsp;06** | Today I **trained my model!!** With only 100 datapoints to train it and a hidden layer of only 100 nodes, it guesses right 6 out of 10 times using the test data. Let's see tomorrow if we can improve on that :) `W00t!` | 8 |
| **S&nbsp;05** | Today I wrote the `train()` method of my `neuralNetwork` class. The last bit of math, where the network learns from errors by updating the weights between the layers, is still fuzzy to me. I've left it like that for now, I need to learn some calculus basics to understand it but wanted to get my hands in the code too badly... Next up: feeding it data to actually train my model! | 7 |
| **S&nbsp;04** | Today I worked more on the **neural network** I started coding last night before bed. https://github.com/rvoulon/my-first-neural-network | 6 |
|            | Also bought a keyboard and mouse and set up a new microSD with Nina for her Piper computer. We set up a new Raspbian install and next we want to do actual Python together. |   |
| **F&nbsp;03** | Today I actually understood something important about **gradient descent**. I'd already seen Andrew Ng's explanation about gradient descent but didn't get the point of it, and didn't understand why you needed it.. Usually gradient descent is explained with a visualization of some equation (with 2 or 3 dimensions), but I never understood why you needed something complicated like gradient descent to get the lowest value of *x* when you can just _point at it in the graph._ Why can't you just simply _solve the equation?_ | 5 |
|            | I finally understand why, it's because what was never mentioned is that those graphs with 2 or 3 dimensions are gross oversimplifications, because the actual equation is _n_-dimensional, it has as many dimensions as there are weights coming into a layer. So it's just utterly impossible to visualize, and mindblowingly difficult to solve. Therefore you need to use gradient descent to "gradually descend" down the slope of the graph until you've got a pretty close guess at the lowest value of x.  | ü§Ø |
| **T&nbsp;02** | ...about **X** = **I** * **W**, where **X** is the matrix of all the moderated signals in a single layer of a neural network. **I** is the matrix of all the input signals coming from the previous layer (or coming into the input layer). **I** is moderated by **W**, the matrix of all the weights between this and the previous layer. After you've calculated **X**, you apply the sigmoid activation function to decide whether the signal is boosted enough to be sent to the next layer: output matrix **O** = sigmoid(**X**) | 4 |
|           | ...and also about **backpropagation**, where an error in the output (as compared to the training data) is used to adjust the weights proportionally, going back layer by layer until all weights have been adjusted. This is how the model is trained on the training data. |  |
| **W&nbsp;01** | ...about **sigmoid functions (logistic function)**, and multiplying **matrices**, and why it makes so much sense to express inputs and weights as matrices! (_Make Your Own Neural Network_)         | 3 |

### July

| Date       | Today I learned...                             | Streak |
|:-----------|:-----------------------------------------------|   |
| **T&nbsp;31**     | ...that a **loss** or **cost function** is really a function to describe the difference (error) between a predicted value and a real world value... | 2 |
|            | ...and that the **learning rate** is actually a fraction that moderates the change in parameters from one training example to the next, so that we don't lose the results from  all the previous training iterations.  (_Make Your Own Neural Network_) |   |
| **M&nbsp;30**     | ...how to set up **Github Pages**          | 1 |
