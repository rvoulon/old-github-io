# Today I learned...

This is a running list to add to daily to track my progress.

## 2018

### August

| Date       | Today I learned...                             | Streak |
|:-----------|:-----------------------------------------------|   |
| **T&nbsp;02** | ...about **X** = **I** * **W**, where **X** is the matrix of all the moderated signals in a single layer of a neural network. **I** is the matrix of all the input signals coming from the previous layer (or coming into the input layer), and it's moderated by **W**, the matrix of all the weights between this and the previous layer. After you've calculated that, you apply the sigmoid function to decide whether the signal is boosted enough to be sent to the next layer. Output matrix **O** = sigmoid(**X**) | 4 |
| **W&nbsp;01** | ...about **sigmoid functions (logistic function)**, and multiplying **matrices**, and why it makes so much sense to express inputs and weights as matrices! (_Make Your Own Neural Network_)         | 3 |

### July

| Date       | Today I learned...                             | Streak |
|:-----------|:-----------------------------------------------|   |
| **T&nbsp;31**     | ...that a **loss** or **cost function** is really a function to describe the difference (error) between a predicted value and a real world value... | 2 |
|            | ...and that the **learning rate** is actually a fraction that moderates the change in parameters from one training example to the next, so that we don't lose the results from  all the previous training iterations.  (_Make Your Own Neural Network_) |   |
| **M&nbsp;30**     | ...how to set up **Github Pages**          | 1 |
